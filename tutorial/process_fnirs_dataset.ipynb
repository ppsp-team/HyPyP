{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Process an open fNIRS dataset\n",
    "\n",
    "**Author**: Guillaume Dumas  \n",
    "**Date**: 2022-11-16  \n",
    "**Note**: This notebook will download ~500MB of data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Import necessary libraries and configure the environment.\n",
    "- Download a large open fNIRS dataset using Pooch.\n",
    "- Uncompress the downloaded archive.\n",
    "- Loop through all dyads in the dataset to check that the recordings of children and parents have the same duration.\n",
    "- Preprocess the fNIRS data (remove short channels, convert raw intensity to optical density, perform quality check, convert to haemoglobin, and filter out heart rate signals).\n",
    "- Create fixed-length epochs from the preprocessed data and import them into HyPyP.\n",
    "- Compute and visualize cross-wavelet coherence between participants.\n",
    "- (Further processing and statistical analysis are indicated to be continued.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "The following cells configure interactive plotting, autoreload, and import the necessary packages for file retrieval, data handling, fNIRS processing using MNE, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inline plotting and autoreloading of modules\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print('Matplotlib inline and autoreload enabled. Warnings suppressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pooch  # For downloading data\n",
    "from zipfile import ZipFile  # For uncompressing archives\n",
    "import os\n",
    "import mne  # MNE for fNIRS processing\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress  # For selecting channels\n",
    "import numpy as np\n",
    "import hypyp  # HyPyP for connectivity and further analyses\n",
    "\n",
    "print('All required libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Whole Dataset\n",
    "\n",
    "The dataset (approx. 500MB) is downloaded using Pooch. For more details on the dataset, see the [Paper](https://doi.org/10.1038/s41597-022-01751-2) and [Data](https://doi.org/10.21979/N9/35DNCW). \n",
    "\n",
    "Pooch documentation: [https://www.fatiando.org/pooch/latest/](https://www.fatiando.org/pooch/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset archive using Pooch\n",
    "file_path = pooch.retrieve(\n",
    "    fname=\"fathers.zip\",\n",
    "    url=\"https://researchdata.ntu.edu.sg/api/access/datafile/91950?gbrecs=true\",\n",
    "    known_hash=\"md5:786e0c13caab4fc744b93070999dff63\",\n",
    "    progressbar=True\n",
    ")\n",
    "\n",
    "print('Dataset downloaded to:', file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncompress the Archive\n",
    "\n",
    "The downloaded data is stored in Pooch's cache folder. We uncompress the ZIP archive to extract the dataset. \n",
    "\n",
    "Refer to the [ZipFile documentation](https://docs.python.org/3/library/zipfile.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the directory of the downloaded file\n",
    "pooch_path = os.path.dirname(file_path)\n",
    "\n",
    "# Uncompress the archive\n",
    "with ZipFile(file_path, 'r') as zip:\n",
    "    zip.extractall(path=pooch_path)\n",
    "    # Use the first file in the archive to set the data path\n",
    "    data_path = os.path.join(pooch_path, zip.filelist[0].filename)\n",
    "\n",
    "print('Data uncompressed. Data path:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Through All Dyads and Check Durations\n",
    "\n",
    "Each dyad in the dataset contains data for a child and their father recorded with NIRx. For each dyad, we load the data for both participants and verify that the duration of the recordings is identical. This check ensures that the subsequent analyses compare synchronized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dyad in os.listdir(data_path):\n",
    "    \n",
    "    # Load the NIRx data for the child\n",
    "    child_path = os.path.join(data_path, dyad, 'child')\n",
    "    hdr_child = [f for f in os.listdir(child_path) if f[-3:] == 'hdr'][0]\n",
    "    raw_child = mne.io.read_raw_nirx(fname=os.path.join(child_path, hdr_child))\n",
    "    \n",
    "    # Load the NIRx data for the parent\n",
    "    parent_path = os.path.join(data_path, dyad, 'parent')\n",
    "    hdr_parent = [f for f in os.listdir(parent_path) if f[-3:] == 'hdr'][0]\n",
    "    raw_parent = mne.io.read_raw_nirx(fname=os.path.join(parent_path, hdr_parent))\n",
    "\n",
    "    # Check that both recordings have the same duration\n",
    "    assert raw_child.times[-1] == raw_parent.times[-1], \"Warning: Files have different durations!\"\n",
    "\n",
    "print('All dyads have matching recording durations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the fNIRS Data\n",
    "\n",
    "The following steps preprocess the fNIRS data for one participant (parent). Similar processing is performed for the child data later.\n",
    "\n",
    "### Remove Short Channels\n",
    "\n",
    "We first remove channels with a source-detector distance that is too short (i.e. less than 0.01 m) using MNE functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fNIRS channels from the parent's data\n",
    "picks = mne.pick_types(raw_parent.info, meg=False, fnirs=True)\n",
    "\n",
    "# Compute source-detector distances\n",
    "dists = mne.preprocessing.nirs.source_detector_distances(raw_parent.info, picks=picks)\n",
    "\n",
    "# Retain only channels with distance greater than 0.01 m\n",
    "raw_parent.pick(picks[dists > 0.01])\n",
    "\n",
    "# Plot the remaining channels (set duration and scalings for visibility)\n",
    "raw_parent.plot(n_channels=len(raw_parent.ch_names), duration=100, scalings='auto', show_scrollbars=False)\n",
    "plt.show()\n",
    "\n",
    "print('Short channels removed from parent data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from Raw Intensity to Optical Density\n",
    "\n",
    "Next, we convert the raw intensity data to optical density. This step is essential for subsequent haemoglobin concentration calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw intensity data to optical density for the parent's data\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_parent)\n",
    "\n",
    "# Plot the optical density data\n",
    "raw_od.plot(n_channels=len(raw_od.ch_names), duration=100, scalings='auto', show_scrollbars=False)\n",
    "plt.show()\n",
    "\n",
    "print('Optical density computed and plotted for parent data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check\n",
    "\n",
    "We assess the quality of the optical density data using the scalp coupling index (SCI). The SCI measures the presence of a prominent synchronous cardiac signal. Channels with SCI below a chosen threshold are marked as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scalp coupling index (SCI) for the parent's optical density data\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "\n",
    "# Plot a histogram of the SCI values\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel='Scalp Coupling Index', ylabel='Count', xlim=[0, 1])\n",
    "plt.show()\n",
    "\n",
    "# Mark channels with SCI below 0.1 as bad\n",
    "raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.1))\n",
    "\n",
    "# Select good channels\n",
    "picks = mne.pick_types(raw_od.info, meg=False, fnirs=True, exclude='bads')\n",
    "raw_od_clean = raw_od.copy().pick(picks)\n",
    "\n",
    "print('Quality check completed; bad channels removed based on SCI.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from Optical Density to Haemoglobin Concentrations\n",
    "\n",
    "Using the Beer-Lambert law, we convert the optical density data into haemoglobin concentrations. The parameter `ppf` is set to 0.1 as an example value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert optical density to haemoglobin concentration for parent's data\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od_clean, ppf=0.1)\n",
    "\n",
    "# Plot the haemoglobin data\n",
    "raw_haemo.plot(n_channels=len(raw_haemo.ch_names), duration=100, scalings='auto', show_scrollbars=False, theme=\"light\")\n",
    "plt.show()\n",
    "\n",
    "print('Conversion to haemoglobin completed and plotted for parent data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Heart Rate from the Signal\n",
    "\n",
    "We inspect the power spectral density (PSD) before filtering, then apply a bandpass filter to remove heart rate components, and finally inspect the PSD again after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the PSD before filtering\n",
    "fig = raw_haemo.compute_psd().plot()\n",
    "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)\n",
    "\n",
    "# Filter the haemoglobin data to remove heart rate (0.05-0.7 Hz)\n",
    "raw_haemo_parent = raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "# Compute and plot the PSD after filtering\n",
    "fig = raw_haemo_parent.compute_psd().plot()\n",
    "fig.suptitle('After filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.show()\n",
    "\n",
    "print('Heart rate removed from parent data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data for the Other Participant (Child)\n",
    "\n",
    "We apply the same preprocessing steps to the child's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process child's data: remove short channels\n",
    "picks = mne.pick_types(raw_child.info, meg=False, fnirs=True)\n",
    "dists = mne.preprocessing.nirs.source_detector_distances(raw_child.info, picks=picks)\n",
    "raw_child.pick(picks[dists > 0.01])\n",
    "\n",
    "# Convert child's data from raw intensity to optical density\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_child)\n",
    "\n",
    "# Quality check for child's data\n",
    "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.1))\n",
    "picks = mne.pick_types(raw_od.info, meg=False, fnirs=True, exclude='bads')\n",
    "raw_od_clean = raw_od.copy().pick(picks)\n",
    "\n",
    "# Convert optical density to haemoglobin for child's data\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od_clean, ppf=0.1)\n",
    "\n",
    "# Filter child's haemoglobin data\n",
    "raw_haemo_child = raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "print('Child data preprocessed: haemoglobin computed and filtered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import into HyPyP\n",
    "\n",
    "We now create fixed-length epochs from the preprocessed haemoglobin data for both the child and parent. Here the epoch duration is set to 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epoch duration (in seconds)\n",
    "duration = 30\n",
    "\n",
    "# Create epochs from the filtered haemoglobin data\n",
    "epo_child = mne.make_fixed_length_epochs(raw_haemo_child, duration=duration, preload=True)\n",
    "epo_parent = mne.make_fixed_length_epochs(raw_haemo_parent, duration=duration, preload=True)\n",
    "\n",
    "print('Fixed-length epochs created for both child and parent.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and Visualize Cross-Wavelet Coherence\n",
    "\n",
    "We compute the cross-wavelet coherence (XWT) between the child and parent epochs. This analysis provides a timeâ€“frequency representation of the inter-brain connectivity.\n",
    "\n",
    "Steps:\n",
    "1. Define frequency bins and compute the frequencies.\n",
    "2. Ensure that both epochs contain the same channels by selecting the intersection of channel names.\n",
    "3. Compute the XWT using HyPyP's `analyses.xwt` function.\n",
    "4. Visualize the result using a 2D image plot with overlaid cone-of-influence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequency parameters\n",
    "freq_bins = 10\n",
    "freqs = np.linspace(0.05, 1.0, freq_bins)\n",
    "sfreq = epo_parent.info['sfreq']\n",
    "\n",
    "# Select common channels between child and parent epochs\n",
    "common_chs = list(set(epo_child.ch_names) & set(epo_parent.ch_names))\n",
    "epo_child = epo_child.pick(common_chs)\n",
    "epo_parent = epo_parent.pick(common_chs)\n",
    "\n",
    "# Compute cross-wavelet coherence between the two sets of epochs\n",
    "data = hypyp.analyses.xwt(epo_child,\n",
    "                           epo_parent,\n",
    "                           freqs=freqs,\n",
    "                           mode='wtc')\n",
    "\n",
    "print('Cross-wavelet coherence computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cross-wavelet coherence for the first channel pair and first epoch\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Average over channels if necessary; here we take the mean over the first two dimensions\n",
    "data_2d = np.abs(data).mean(axis=(0, 1))[0, :, :]\n",
    "\n",
    "plt.imshow(data_2d,\n",
    "           aspect='auto',\n",
    "           cmap='plasma',\n",
    "           interpolation='lanczos')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel('Frequencies (Hz)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.yticks(range(0, freq_bins, 2), np.round(100*freqs[range(0, freq_bins, 2)])/100)\n",
    "plt.ylim([0, freq_bins-1])\n",
    "\n",
    "smax = data.shape[-1]\n",
    "plt.xlim([0, smax])\n",
    "plt.xticks(np.arange(0, smax+sfreq, sfreq), range(duration+1))\n",
    "\n",
    "# Calculate cone-of-influence (COI) boundaries\n",
    "coi = 2.5 * sfreq / freqs\n",
    "rev_coi = data.shape[-1] - coi\n",
    "idx = np.arange(len(freqs))\n",
    "plt.plot(coi, idx, 'w')\n",
    "plt.plot(data.shape[-1]-coi, idx, 'w')\n",
    "plt.fill_between(coi, idx, hatch='X', fc='w', alpha=0.3)\n",
    "plt.fill_between(rev_coi, idx, hatch='X', fc='w', alpha=0.3)\n",
    "\n",
    "plt.axvspan(0, min(coi), hatch='X', fc='w', alpha=0.3)\n",
    "plt.axvspan(smax, max(rev_coi), hatch='X', fc='w', alpha=0.3)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print('Cross-wavelet coherence plot displayed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, Let's Process All the Dataset\n",
    "\n",
    "The following cell outlines how you might loop over all dyads in the dataset to perform preprocessing and connectivity analyses. In practice, you could store the results in a Pandas dataframe for further statistical analysis.\n",
    "\n",
    "> Note: This section is a placeholder and should be expanded based on your processing needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dyad in os.listdir(data_path):\n",
    "    \n",
    "    # Load NIRx data for child\n",
    "    child_path = os.path.join(data_path, dyad, 'child')\n",
    "    hdr_child = [f for f in os.listdir(child_path) if f[-3:] == 'hdr'][0]\n",
    "    raw_child = mne.io.read_raw_nirx(fname=os.path.join(child_path, hdr_child))\n",
    "    \n",
    "    # Load NIRx data for parent\n",
    "    parent_path = os.path.join(data_path, dyad, 'parent')\n",
    "    hdr_parent = [f for f in os.listdir(parent_path) if f[-3:] == 'hdr'][0]\n",
    "    raw_parent = mne.io.read_raw_nirx(fname=os.path.join(parent_path, hdr_parent))\n",
    "    \n",
    "    # Preprocessing steps would go here\n",
    "    \n",
    "    # For example, compute connectivity and store in a DataFrame\n",
    "    # (This is a placeholder for your processing code)\n",
    "    pass\n",
    "\n",
    "print('Processing of all dyads completed (placeholder).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis and Visualization\n",
    "\n",
    "> **To be continued...**\n",
    "\n",
    "Further steps would include performing statistical analyses on the computed connectivity metrics and visualizing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "92430349c42c9c2206b2d41976facb0f37f2bbc315dbacad8335dce99b787c6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
